import gradio as gr
import torch
import os
from model import GPTLanguageModel
from config import Config
from dataset import BPEDataset

# -----------------------------------------------------------------------------
# 1. å‡†å¤‡å·¥ä½œï¼šåŠ è½½æ¨¡å‹ (åªåœ¨å¯åŠ¨æ—¶è¿è¡Œä¸€æ¬¡)
# -----------------------------------------------------------------------------
print("æ­£åœ¨å¯åŠ¨æœåŠ¡ï¼Œè¯·ç¨å€™...")
device = 'cpu' # å¦‚æœä½ æœ‰ GPU å¯ä»¥æ”¹æˆ 'cuda'
config = Config()

# è·¯å¾„è®¾ç½® (ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œç¡®ä¿åœ¨ä»»ä½•åœ°æ–¹è·‘éƒ½ä¸ä¼šé”™)
base_dir = os.path.dirname(os.path.abspath(__file__))
model_path = os.path.join(base_dir, 'models', 'mini_gpt_step_5000.pth') # ç¡®ä¿æ–‡ä»¶åå¯¹
data_path = os.path.join(base_dir, 'data', 'mini_gpt', 'input.txt')

# åŠ è½½åˆ†è¯å™¨ (Tokenizer)
print("Loading Tokenizer...")
dataset = BPEDataset(config, data_path)

# åˆå§‹åŒ–æ¨¡å‹éª¨æ¶
print("Loading Model...")
model = GPTLanguageModel(config)

# åŠ è½½æƒé‡ (ä½¿ç”¨ä¹‹å‰ä¿®å¤è¿‡çš„ä¸‡èƒ½åŠ è½½é€»è¾‘)
checkpoint = torch.load(model_path, map_location=device)
if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
    state_dict = checkpoint['model_state_dict']
elif isinstance(checkpoint, dict) and 'model' in checkpoint:
    state_dict = checkpoint['model']
else:
    state_dict = checkpoint
    
# å»æ‰å¤šå¡è®­ç»ƒçš„ module. å‰ç¼€
new_state_dict = {}
for k, v in state_dict.items():
    if k.startswith('module.'):
        new_state_dict[k[7:]] = v
    else:
        new_state_dict[k] = v

model.load_state_dict(new_state_dict)
model.to(device)
model.eval()
print("Model loaded successfully!")

# -----------------------------------------------------------------------------
# 2. å®šä¹‰æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
# -----------------------------------------------------------------------------
def generate_text(prompt, temperature, top_k):
    """
    è¿™æ˜¯ Gradio æŒ‰é’®èƒŒåçš„é€»è¾‘ï¼š
    è¾“å…¥ï¼šæç¤ºè¯, æ¸©åº¦, Top-K
    è¾“å‡ºï¼šç”Ÿæˆçš„æ–‡æœ¬
    """
    if not prompt.strip():
        return "âš ï¸ è¯·è¾“å…¥ä¸€ç‚¹æç¤ºè¯..."

    # 1. ç¼–ç  (è®°å¾—åŠ  unsqueeze å˜æˆ 2D)
    context = torch.tensor(dataset.encode(prompt), dtype=torch.long, device=device).unsqueeze(0)
    
    # 2. ç”Ÿæˆ (å¤ç”¨ä½ å†™å¥½çš„ generate)
    # è¿™é‡Œçš„ max_new_tokens å¯ä»¥è°ƒï¼Œæ¯”å¦‚ç”Ÿæˆé•¿ä¸€ç‚¹
    generated_ids = model.generate(context, max_new_tokens=200, temperature=temperature, top_k=int(top_k))
    
    # 3. è§£ç 
    output_text = dataset.decode(generated_ids[0].tolist())
    return output_text

# -----------------------------------------------------------------------------
# 3. æ­å»ºç½‘é¡µç•Œé¢ (UI Layout)
# -----------------------------------------------------------------------------
with gr.Blocks(title="Mini-GPT Playground") as demo:
    gr.Markdown("# ğŸ¤– Mini-GPT: Shakespeare Edition")
    gr.Markdown("è¿™æ˜¯ä¸€ä¸ªåŸºäº Transformer æ¶æ„ä»é›¶è®­ç»ƒçš„å¾®å‹ GPT æ¨¡å‹ï¼Œç»è¿‡äº† 5000 æ­¥çš„èå£«æ¯”äºšå…¨é›†è®­ç»ƒã€‚")
    gr.Markdown("Created by minmin-winter")
    
    with gr.Row():
        with gr.Column():
            # å·¦è¾¹ï¼šè¾“å…¥åŒº
            input_box = gr.Textbox(label="è¾“å…¥æç¤ºè¯ (Prompt)", placeholder="ä¾‹å¦‚: The King said", lines=2)
            
            # ä¸¤ä¸ªæ»‘å—ï¼šæ§åˆ¶å‚æ•°
            temp_slider = gr.Slider(minimum=0.1, maximum=2.0, value=0.8, step=0.1, label="Temperature (æ¸©åº¦ - åˆ›é€ åŠ›)")
            topk_slider = gr.Slider(minimum=1, maximum=100, value=50, step=1, label="Top-K (é‡‡æ ·èŒƒå›´)")
            
            # æŒ‰é’®
            generate_btn = gr.Button("ğŸš€ å¼€å§‹ç”Ÿæˆ", variant="primary")
            
        with gr.Column():
            # å³è¾¹ï¼šè¾“å‡ºåŒº
            output_box = gr.Textbox(label="æ¨¡å‹ç”Ÿæˆçš„ç»­å†™", lines=10, interactive=False)

    # ç»‘å®šäº‹ä»¶ï¼šç‚¹æŒ‰é’® -> è¿è¡Œå‡½æ•° -> æ›´æ–°è¾“å‡º
    generate_btn.click(fn=generate_text, inputs=[input_box, temp_slider, topk_slider], outputs=output_box)

# -----------------------------------------------------------------------------
# 4. å¯åŠ¨ï¼
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    demo.launch(share=True)