# Mini-GPT: ä»é›¶æ„å»ºçš„å­—ç¬¦çº§ç”Ÿæˆæ¨¡å‹ ğŸš€

è¿™æ˜¯ä¸€ä¸ªåŸºäº PyTorch ä»é›¶å®ç°çš„ GPT (Generative Pre-trained Transformer) æ¨¡å‹ã€‚æœ¬é¡¹ç›®ä¸ä¾èµ–é«˜çº§å°è£…åº“ï¼ˆå¦‚ HuggingFaceï¼‰ï¼Œæ—¨åœ¨é€šè¿‡æ‰‹å†™æ¯ä¸€è¡Œä»£ç ï¼Œæ·±å…¥ç†è§£ Transformer çš„åº•å±‚åŸç†ï¼ˆAttention, LayerNorm, Residual Connectionsï¼‰ã€‚

ç›®å‰æ¨¡å‹åœ¨ **Tiny Shakespeare** æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œèƒ½å¤Ÿç”Ÿæˆç±»ä¼¼èå£«æ¯”äºšé£æ ¼çš„å¤è‹±è¯­æ–‡æœ¬ã€‚

## ğŸ“‚ é¡¹ç›®ç»“æ„ (Project Structure)

æœ¬é¡¹ç›®é‡‡ç”¨äº†æ ‡å‡†çš„å·¥ç¨‹åŒ–åˆ†å±‚ç»“æ„ï¼š

* **`model.py`**: æ¨¡å‹æ ¸å¿ƒå®šä¹‰ (å« Multi-Head Attention, FeedForward, Transformer Block)ã€‚
* **`train.py`**: è®­ç»ƒè„šæœ¬ (åŒ…å«è®­ç»ƒå¾ªç¯ã€Loss ç›‘æ§ã€æ¨¡å‹å®šæœŸä¿å­˜)ã€‚
* **`dataset.py`**: æ•°æ®å¤„ç†æ¨¡å— (è‡ªå®šä¹‰ PyTorch Datasetï¼Œå¤„ç†å­—ç¬¦çº§ Tokenization)ã€‚
* **`config.py`**: é…ç½®ä¸­å¿ƒ (é›†ä¸­ç®¡ç†è¶…å‚æ•°ï¼Œå¦‚ learning_rate, batch_size ç­‰)ã€‚
* **`inference.py`**: æ¨ç†è„šæœ¬ (åŠ è½½è®­ç»ƒå¥½çš„æƒé‡å¹¶ç”Ÿæˆæ–‡æœ¬)ã€‚
* **`data/`**: å­˜æ”¾è®­ç»ƒæ•°æ® (å¦‚ `input.txt`)ã€‚

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹ (Quick Start)

### 1. å®‰è£…ä¾èµ–

ç¡®ä¿ä½ çš„ç¯å¢ƒä¸­æœ‰ PyTorch å’Œ NumPyï¼š

    pip install -r requirements.txt

### 2. å¼€å§‹è®­ç»ƒ

è¿è¡Œè®­ç»ƒè„šæœ¬ã€‚è„šæœ¬ä¼šè‡ªåŠ¨åŠ è½½ `data/` ä¸‹çš„æ•°æ®å¹¶å¼€å§‹è®­ç»ƒã€‚

    python 05_transformer_project/train.py

### 3. ç”Ÿæˆæ–‡æœ¬ (æ¨¡å‹æ¨ç†)

è®­ç»ƒå®Œæˆåï¼ˆé»˜è®¤ 5000 æ­¥ï¼‰ï¼Œè¿è¡Œæ¨ç†è„šæœ¬æ¥æŸ¥çœ‹æ•ˆæœï¼š

    python 05_transformer_project/inference.py

## ğŸ“Š è®­ç»ƒæ•ˆæœ

ç»è¿‡ **5000 step** çš„è®­ç»ƒï¼Œæ¨¡å‹åœ¨éªŒè¯é›†ä¸Šè¾¾åˆ°äº† **1.78** çš„ Lossã€‚
ç”Ÿæˆçš„æ–‡æœ¬ç¤ºä¾‹ï¼š

> **ORCULIO:**  
> Yen mystry; peasince  
> To pron the dudgeth Rombeash so?  
> ...

## ğŸ§  æ ¸å¿ƒçŸ¥è¯†ç‚¹

* **ä»é›¶æ‰‹å†™**ï¼šæ‰‹åŠ¨å®ç°äº† Causal Self-Attention å’Œ Multi-Head Attentionã€‚
* **å·¥ç¨‹åŒ–é‡æ„**ï¼šå°†å­¦ä¹ é˜¶æ®µçš„å•æ–‡ä»¶è„šæœ¬é‡æ„ä¸ºæ¨¡å—åŒ–çš„å·¥ç¨‹é¡¹ç›®ã€‚
* **è®­ç»ƒç®¡ç†**ï¼šå®ç°äº† checkpoint ä¿å­˜ä¸åŠ è½½æœºåˆ¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­ç»ƒã€‚

---
Created by Minmin-winter | DL Learning Path
