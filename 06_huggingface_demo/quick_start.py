import os
# è®¾ç½® HF é•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# ==========================================
# 1. åŠ è½½â€œå†›ç«â€ï¼šTokenizer å’Œ Model
# ==========================================
print("æ­£åœ¨è¿æ¥ Hugging Face Hub ä¸‹è½½/åŠ è½½ GPT-2...")

# from_pretrained æ˜¯ Hugging Face æœ€æ ¸å¿ƒçš„é­”æ³•
# å®ƒä¼šè‡ªåŠ¨å»å®˜ç½‘æ‰¾å« "gpt2" çš„æ¨¡å‹ï¼Œä¸‹è½½é…ç½®æ–‡ä»¶ã€è¯è¡¨å’Œæƒé‡
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

print("æ¨¡å‹åŠ è½½å®Œæ¯•ï¼å‡†å¤‡èµ·é£ï¼ğŸš€")

# ==========================================
# 2. å‡†å¤‡è¾“å…¥
# ==========================================
# æˆ‘ä»¬å¯ä»¥è¯•è¯•ç»™å®ƒä¸€ä¸ªæ›´éš¾çš„ Promptï¼Œçœ‹å®ƒæ‡‚ä¸æ‡‚å¸¸è¯†
text = "The scientist discovered a new planet that"
print(f"\n[ç”¨æˆ·è¾“å…¥]: {text}")

# ç¼–ç  (è¿™å°±ç›¸å½“äºä½ å†™çš„ dataset.encode + unsqueeze)
# return_tensors='pt' è¡¨ç¤ºç›´æ¥è¿”å› PyTorch çš„ Tensor
inputs = tokenizer(text, return_tensors="pt")

# ==========================================
# 3. ç”Ÿæˆ (Inference)
# ==========================================
# è®¾ä¸ºè¯„ä¼°æ¨¡å¼ (ä¸è®¡ç®—æ¢¯åº¦ï¼Œçœå†…å­˜)
model.eval()

print("æ­£åœ¨ç”Ÿæˆä¸­...")
with torch.no_grad():
    # è¿™é‡Œçš„å‚æ•°æ˜¯ä¸æ˜¯å¾ˆçœ¼ç†Ÿï¼Ÿ
    # do_sample=True: å¼€å¯éšæœºé‡‡æ · (Temperatureæ‰æœ‰æ•ˆ)
    # temperature=0.7: ç¨å¾®æœ‰ç‚¹åˆ›é€ åŠ›ï¼Œä½†ä¸è¦å¤ªç–¯
    # top_k=50: åªçœ‹å‰50ä¸ªæ¦‚ç‡æœ€é«˜çš„è¯
    # max_length=100: ç”Ÿæˆçš„æ€»é•¿åº¦
    outputs = model.generate(
        inputs["input_ids"], 
        max_length=100, 
        do_sample=True, 
        temperature=0.7, 
        top_k=50,
        pad_token_id=tokenizer.eos_token_id # é¿å…è­¦å‘Š
    )

# ==========================================
# 4. è§£ç 
# ==========================================
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("-" * 30)
print("[GPT-2 ç”Ÿæˆç»“æœ]:")
print(generated_text)
print("-" * 30)