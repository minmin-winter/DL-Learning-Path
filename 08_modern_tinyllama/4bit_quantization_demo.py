"""
[Colab ä¸“ç”¨è„šæœ¬]
æ³¨æ„ï¼šæ­¤è„šæœ¬éœ€è¦ NVIDIA æ˜¾å¡ + bitsandbytes åº“æ”¯æŒï¼Œå»ºè®®åœ¨ Colab T4 ç¯å¢ƒè¿è¡Œã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä½¿ç”¨ bitsandbytes è¿›è¡Œ 4-bit é‡åŒ–åŠ è½½ (QLoRA åŸºç¡€)ã€‚
2. ä½¿ç”¨ apply_chat_template å¤„ç†å¯¹è¯æ ¼å¼ã€‚
3. ä½“éªŒ T4 GPU çš„æ¨ç†é€Ÿåº¦ã€‚
"""
# ... ä¸‹é¢ç²˜è´´ä½ çš„ä»£ç  ...
# 1. å®‰è£…å¿…è¦çš„åº“ (Colabé‡Œè¦åŠ æ„Ÿå¹å·è¿è¡Œå‘½ä»¤)
# bitsandbytes æ˜¯é‡åŒ–æ ¸å¿ƒåº“ï¼Œaccelerate æ˜¯åŠ é€Ÿåº“
# !pip install -q transformers accelerate bitsandbytes

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# 2. è®¾ç½®æ¨¡å‹ ID
model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# 3. ğŸ”¥ æ ¸å¿ƒç§‘æŠ€ï¼š4-bit é‡åŒ–é…ç½® (QLoRA çš„åŸºç¡€)
# è¿™ä¼šè®©æ¨¡å‹ä½“ç§¯ç¼©å° 4 å€ï¼Œé€Ÿåº¦é£å¿«ï¼Œä¸”æ˜¾å­˜å ç”¨æä½
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

print(f"æ­£åœ¨ä¸‹è½½å¹¶ä»¥ 4-bit é‡åŒ–åŠ è½½æ¨¡å‹: {model_id} ...")

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config, # ğŸ‘ˆ åº”ç”¨é‡åŒ–é…ç½®
    device_map="auto"               # è‡ªåŠ¨ä¸¢ç»™ GPU
)

# 4. å‡†å¤‡å¯¹è¯ (Chat Template)
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Explain Quantum Mechanics to a 5-year-old in simple English."},
]

prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

# 5. ç”Ÿæˆ
print("\næ­£åœ¨æ€è€ƒä¸­... (æ„Ÿå—ä¸€ä¸‹ GPU çš„é€Ÿåº¦)")
inputs = tokenizer(prompt, return_tensors="pt").to("cuda") # ğŸ‘ˆ ä¸¢åˆ° CUDA (GPU) ä¸Š

model.eval()
with torch.no_grad():
    outputs = model.generate(
        inputs.input_ids,
        max_new_tokens=200,
        do_sample=True,
        temperature=0.7,
        top_k=50,
        top_p=0.95
    )

response = outputs[0][inputs.input_ids.shape[1]:]
print("\nğŸ¤– [TinyLlama @ GPU]:")
print(tokenizer.decode(response, skip_special_tokens=True))