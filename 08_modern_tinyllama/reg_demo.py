"""
[Colab ä¸“ç”¨è„šæœ¬] TinyLlama RAG æ¼”ç¤º
æ³¨æ„ï¼šæ­¤è„šæœ¬éœ€è¦ T4 GPU ç¯å¢ƒ + bitsandbytes åº“ã€‚

æµç¨‹ï¼š
1. è¯»å–æœ¬åœ° LEARNING_LOG.md
2. åˆ‡åˆ†å¹¶å­˜å…¥ Chroma å‘é‡åº“
3. åŠ è½½ TinyLlama-1.1B (4-bit é‡åŒ–)
4. æ£€ç´¢ + ç”Ÿæˆå›ç­”
"""

import torch
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# ===================== 1. å‡†å¤‡çŸ¥è¯†åº“ =====================
print("æ­£åœ¨å¤„ç†çŸ¥è¯†åº“...")
# âš ï¸ æ³¨æ„ï¼šåœ¨ Colab è¿è¡Œæ—¶ï¼Œéœ€è¦å…ˆä¸Šä¼ è¿™ä¸ªæ–‡ä»¶
try:
    loader = TextLoader("./LEARNING_LOG.md", encoding="utf-8")
    documents = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_documents(documents)
    
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    db = Chroma.from_documents(chunks, embedding_model)
    print(f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼å…± {len(chunks)} ä¸ªåˆ‡ç‰‡ã€‚")
except Exception as e:
    print(f"âŒ çŸ¥è¯†åº“åŠ è½½å¤±è´¥ (å¯èƒ½æ˜¯æ–‡ä»¶æ²¡ä¸Šä¼ ): {e}")
    exit()

# ===================== 2. åŠ è½½æ¨¡å‹ (4-bit) =====================
print("\næ­£åœ¨åŠ è½½ TinyLlama (4-bit)...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4"
)
model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map="auto")

# ===================== 3. RAG æ ¸å¿ƒå‡½æ•° =====================
def ask_with_rag(question):
    print(f"\nğŸ¤” ç”¨æˆ·æé—®: {question}")
    
    # æ£€ç´¢
    docs = db.similarity_search(question, k=3)
    context = "\n".join([d.page_content for d in docs])
    
    # æ„é€  Prompt (ä¸­æ–‡ä¼˜åŒ–ç‰ˆ)
    messages = [
        {
            "role": "system", 
            "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢çš„ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœå‚è€ƒèµ„æ–™é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œå°±è¯´'æˆ‘ä¸çŸ¥é“'ã€‚è¯·åŠ¡å¿…ç”¨ä¸­æ–‡å›ç­”ã€‚"
        },
        {
            "role": "user", 
            "content": f"ã€å‚è€ƒèµ„æ–™ã€‘:\n{context}\n\nã€é—®é¢˜ã€‘: {question}"
        }
    ]
    
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids, 
            max_new_tokens=200, 
            temperature=0.7,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.eos_token_id
        )
        
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    return response

# ===================== 4. æµ‹è¯• =====================
if __name__ == "__main__":
    q = "Minmin-winter åˆ›å»ºäº†ä»€ä¹ˆè´¦å·ï¼Ÿ"
    ans = ask_with_rag(q)
    print(f"\nğŸ¤– TinyLlama å›ç­”:\n{ans}")