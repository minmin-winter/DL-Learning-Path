import os
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from peft import PeftModel # ğŸ‘ˆ å…³é”®è§’è‰²ï¼šè´Ÿè´£æ‹¼è£…æ¨¡å‹

# 1. è·¯å¾„è®¾ç½®
# æŒ‡å‘åŸæœ¬çš„ GPT-2
base_model_path = "gpt2" 
# æŒ‡å‘ä½ åˆšæ‰è®­å¥½çš„ LoRA æƒé‡æ–‡ä»¶å¤¹, è‡ªåŠ¨å¯»æ‰¾æœ€æ–°çš„ checkpoint 
checkpoints = [d for d in os.listdir("./models") if d.startswith("checkpoint")]
if checkpoints:
    latest = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))[-1]
    lora_path = os.path.join("./models", latest)
    print(f"è‡ªåŠ¨å®šä½åˆ°æœ€æ–°æƒé‡: {lora_path}")
else:
    print("âŒ æ²¡æ‰¾åˆ°è®­ç»ƒå¥½çš„ LoRA æƒé‡ï¼è¯·æ£€æŸ¥è·¯å¾„ã€‚")
    exit()

# 2. åŠ è½½â€œç´ ä½“â€æ¨¡å‹ (Base Model)
print(f"æ­£åœ¨åŠ è½½ç´ ä½“æ¨¡å‹ GPT-2...")
base_model = GPT2LMHeadModel.from_pretrained(base_model_path)
tokenizer = GPT2Tokenizer.from_pretrained(base_model_path)
tokenizer.pad_token = tokenizer.eos_token

# 3. ğŸ’¥ åˆä½“ï¼åŠ è½½ LoRA å¤–æŒ‚
print(f"æ­£åœ¨æŒ‚è½½ LoRA å¤–æŒ‚...")
# PeftModel.from_pretrained ä¼šè‡ªåŠ¨æŠŠ LoRA æƒé‡â€œåŠ â€åˆ° base_model ä¸Š
model = PeftModel.from_pretrained(base_model, lora_path)

# 4. çœ‹çœ‹æ•ˆæœ
prompt = "å¤–å–"
print(f"\n[ç”¨æˆ·è¾“å…¥]: {prompt}")
inputs = tokenizer(prompt, return_tensors="pt")

model.eval()
print("æ­£åœ¨ç”Ÿæˆä¸­...")

with torch.no_grad():
    outputs = model.generate(
        input_ids = inputs["input_ids"], 
        attention_mask = inputs["attention_mask"],
        max_length=50, 
        do_sample=True, 
        temperature=0.7, 
        top_k=50,
        pad_token_id=tokenizer.eos_token_id
    )

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("-" * 30)
print("[LoRA GPT-2 ç”Ÿæˆç»“æœ]:")
print(generated_text)
print("-" * 30)