from datasets import load_dataset
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling
# ğŸ‘‡ æ–°å¼•å…¥çš„åº“ï¼šPEFT (Parameter-Efficient Fine-Tuning)
from peft import LoraConfig, get_peft_model, TaskType

# 1. å‡†å¤‡æ•°æ® (è·Ÿæ˜¨å¤©ä¸€æ ·ï¼Œåªå–å‰100æ¡åšæ¼”ç¤º)
print("æ­£åœ¨åŠ è½½æ•°æ®...")
dataset = load_dataset("XiangPan/waimai_10k", split="train[:100]")

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

def process_function(examples):
    tokens = tokenizer(examples["text"], padding="max_length", truncation=True, max_length=64)
    tokens["labels"] = tokens["input_ids"].copy()
    return tokens

tokenized_datasets = dataset.map(process_function, batched=True)

# 2. åŠ è½½æ¨¡å‹
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
model = GPT2LMHeadModel.from_pretrained("gpt2")

# ====================================================
# ğŸ”¥ğŸ”¥ğŸ”¥ LoRA æ ¸å¿ƒé…ç½® (é­”æ³•å‘ç”Ÿçš„åœ°æ–¹) ğŸ”¥ğŸ”¥ğŸ”¥
# ====================================================
print("\næ­£åœ¨ç»™æ¨¡å‹æŒ‚è½½ LoRA å¤–æŒ‚...")

# å®šä¹‰é…ç½®
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, # ä»»åŠ¡ç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹ (GPTç³»åˆ—)
    inference_mode=False,         # è®­ç»ƒæ¨¡å¼
    r=8,                          # Rank (ç§©)ï¼šè¿™ä¸ªæ•°è¶Šå°ï¼Œå‚æ•°è¶Šå°‘ã€‚é€šå¸¸ 8, 16, 32
    lora_alpha=32,                # ç¼©æ”¾ç³»æ•°ï¼Œé€šå¸¸æ˜¯ r çš„ 2 å€æˆ– 4 å€
    lora_dropout=0.1,             # é˜²æ­¢è¿‡æ‹Ÿåˆ
    # âš ï¸ å…³é”®ï¼šæˆ‘ä»¬è¦å»å½±å“æ¨¡å‹é‡Œçš„å“ªä¸€å±‚ï¼Ÿ
    # å¯¹äº GPT-2ï¼Œæ³¨æ„åŠ›å±‚çš„åå­—é€šå¸¸å« 'c_attn'ã€‚
    # å¯¹äº Llamaï¼Œé€šå¸¸æ˜¯ 'q_proj', 'v_proj'ã€‚
    target_modules=["c_attn"]     
)

# ğŸª„ å˜èº«ï¼æŠŠæ™®é€šæ¨¡å‹å˜æˆ LoRA æ¨¡å‹
model = get_peft_model(model, peft_config)

# ğŸ–¨ï¸ æ‰“å°ä¸€ä¸‹ï¼Œçœ‹çœ‹æˆ‘ä»¬çœäº†å¤šå°‘å‚æ•°ï¼
print("="*50)
model.print_trainable_parameters()
print("="*50)

# ====================================================

# 3. è®­ç»ƒå‚æ•° (è·Ÿæ˜¨å¤©ä¸€æ ·)
training_args = TrainingArguments(
    output_dir="./models/",
    per_device_train_batch_size=4,
    num_train_epochs=50,          # LoRA æ”¶æ•›ç¨æ…¢ï¼Œæˆ–è€…éœ€è¦å¤šè·‘å‡ è½®ï¼Œè¿™é‡Œè®¾50ä¿è¯å­¦ä¼š
    logging_steps=10,
    save_steps=100,
    learning_rate=1e-3,           # âš ï¸ æ³¨æ„ï¼šLoRA çš„å­¦ä¹ ç‡é€šå¸¸æ¯”å…¨é‡å¾®è°ƒè¦å¤§ (1e-3 vs 1e-5)
    use_cpu=True,
)

# 4. å¼€å§‹è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

print("å¼€å§‹ LoRA å¾®è°ƒ...")
trainer.train()