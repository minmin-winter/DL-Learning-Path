
# 📔 Minmin-winter's Deep Learning Journey

> 记录我从零开始死磕深度学习的每一天.  
> "Slow is smooth, smooth is fast."

## 📆 2025-12-20 Sat. | Day 6: 变故中的思考，修整

**mood**: 😢

### ☁️ 碎碎念

今天之前一直用的Gemini对话框崩了，之前所有的对话记录他都找不到了，现在和他对话像是在和失忆的人交流一样。没办法，简单收拾一下心情之后，只好重新再开一个对话框学习，还需要把之前的信息都同步起来，正好也给自己一个休息的时间吧，三周以来，也算学得蛮多了。  
今天还突然产生了以后要不要学AI4S的想法，简单的以为那边会不会不卷一些，稳定一些😆(哈哈哈)，了解了一些之后还是打消了这个念头,确实跟着时代风口走感觉更适合我一些，继续加油吧! 💪 auto driving, embodied AI, VLA, MMLM都可能是以后的方向，i先继续加油学着吧！

## 📆 2025-12-19 Fri. | Day 5: 学了transformer大部分

### 📝 核心进度

- 掌握了self-attention, multi-head attention机制
- 初步学习了positional_encoding,虽然原理有点没看懂，哈哈，但被直接加到token_embedding的思想震撼，更加深刻理解了映射到高维空间的思想
- 尝试搭建了transformer block和简易的transformer模型

### 💡 思考

今天更加理解了transformer的优点不仅在于通过**multi-head机制**以及变换成**矩阵运算**来充分开发适配GPU,也在于通过计算全局attention机制，来高效利用，理解全局，相较于RNN和LSTM走一步看一步有效延长文本有效长度,提升理解效果

## 📆 2025-12-17 Wed. | Day 4: 学完RNN与工程能力磨练

### 📝 核心进度

- Ubantu换了Fcitx5输入法，感觉确实比原装输入法好用一些
- VS code实现了按Esc自动切换英文，并继续精通了a, i, >C+`等无鼠标操作
- 实现了RNN,明白了文本->ID->tensor->Embedding->RNN->Linear
- 入门了传说中的Transformer,了解了self-attention的数学原理

### ☁️ 碎碎念

比起完成了RNN的学习，我觉得今天更大的收获是第一次开始更加理解与重视工程能力，更加重视输入，重视无键盘流，重视文件结构，重视代码规范。同时，这也让我意识到未来还有很长的道路需要走，不管是基本的算法，工程能力，代码能力，对领域的理解和洞察力，这些都是我未来许许多多需要去学习的地方。明天开始，就要去学习神作Transformer了，加油啊！ 💪

## 📅 2025-12-15 Mon. | Day 3: 跑通CNN与ResNet

### 📝 核心进度

- 理解了卷积层(提取特征)和全连接层(找全局关系)之间的区别和联系
- 学习了Residual Block的代码和基本思想
- 领会了厚度，特征，channels，卷积核之间的联系

💡 思考

- 有多少个out_channels就会有多少个卷积核。每个卷积核对应一个out_channel，一个特征，一种与其他卷积核不同的加权算法。
- 而卷积核用来加权的原材料就是每一个in_channels，并根据kernel_size和padding来看是否进行浓缩和尺寸变化。并且kernel_size和padding影响的只是单个channel，单个特征的精细程度。
- 对于第一层卷积层in_channel=1或3的理解，用这个厚度来理解也很好，因为才是第一层，所以也谈不上什么特征，或者说特征就是图片一个本身，而彩色图片也就算是3个图片自己的叠加而已

☁️ 碎碎念

今天终于算是学完了Residual Block，CV基础的学习也告一段落了，接下来就要进入NLP了。离学期结束还有两个星期，加油啊！💪  
今天在学习过程中还遇到了两个之前没有体会过的体验，一个就是第一次觉得模型训练的时长太久了，导致无法很好的掌控这段时间了，以后也要多家思考如何利用，可以在终端里训练，然后继续做事：另一个是意识到自己的Thinkbook 16+ 还是以CPU见长，GPU性能并不出众，以后可能要考虑租卡之类的了，现在先凑合着用吧。

## 📅 2025-12-14 Sun. | Day 2: 初识 CNN 与工程化整理

**Mood**: 😤 -> 😌 (折腾完舒服了)  

### 📝 核心进度

- 明白了全连接层的局限性（破坏空间结构）。
- 了解了卷积核（Kernel）就像手电筒一样扫描图片。
- 把 GitHub 仓库整理得像个样板房了。

### 💡 思考

原来 CNN 的核心就是“不拍扁”，保持图片的二维形状去提取特征。感觉这更符合人类的视觉逻辑。

### ☁️ 碎碎念

今天同样也是一个milestone。我创建了我的Github账号，并开始系统整理我的仓库，即使现在文件，代码量都并不多，但也已经感受到整理仓库真的十分劳神了。不过能够看着仓库变得清清爽爽，感觉做这些都是值得的。 😄  
同时我也新学习了CNN里的卷积层，池化层，初步感知了CNN的核心思想:用卷积层，池化层等工具不断提炼，优化二维图形的信息，并凭借最终高度浓缩的信息特征来推断输入图形.明天将开始写CNN在MNIST上的main.py了，一定可以完成的！ 💪

## 📅 2025-12-13 Sat. | Day 1: 搞定 MNIST 全连接网络

**Mood**: 🤩 (激动)

### 📝 核心进度

- 跑通了人生第一个神经网络(MLP)，准确率 95.19%！
- 明白了梯度清零，前向传播，计算损失，后向传播，更新参数这一套完整模型训练流程
- 彻底搞懂了 `view(-1, 28*28)` 的物理含义。

### 🐛 踩坑实录 (Bug Log)

- **问题**：`ImportError: cannot import name 'imshow'`
- **原因**：笑死，写完代码没保存 (`Ctrl+S`) 就运行了，Python 读的是空文件。
- **教训**：以后运行前先看一眼标签页有没有圆点！一定要养成肌肉记忆。

### 💭 碎碎念 (Dev Diary)

这是我选择开始Learning Log的的第一天，在这周的前一周，我入门了Ubantu，bash,vim,git等工具类，,且在本周的前段时间学习了numpy基础语法并正式入门DL，从加载数据开始学起。  
而今天这一天确实值得作为我的一个milestone了。我融汇前面所学，通过两层全连接层，加一层激活函数的极简模型，以最终95.19%的准确率完成了MINST数字识别。从此也算是可以说出"Hello DL!"了.第二天就要开始继续更深层次的学习了，继续学习CNN，也是个重重点，加油啊！💪
