
# 📔 Minmin-winter's Deep Learning Journey

> 记录我从零开始死磕深度学习的每一天.  
> "Slow is smooth, smooth is fast."
## 📆 2025-12-27 Sat. | Day 10: 初探Hugging Face生态

### 📝 核心进度

- 使用了Hugging Face的工具，完成了加载外卖评论数据集的操作
- 初探Transfomers, Datasets, Trainer等库， 感受到了HF封装完备的生态的强大

### 🐛 Bug Logs

- 无法解析导入...  ->  此环境下还没有安装这个库 -> pip install ...
- max Retry[1/5] ->  网络连接出问题 -> 换镜像源等

### 💡 思考

可以明显的感受到差别学习流程上的差别，之前是from scratch, 现在是利用封装完好的Hugging Face生态来进行调包，更多的是学习流程以及data flow, 也或许这才是以后的理想状态了吧

### ☁️ 碎碎念

虽然今天很不适应学习方式，流程上的转变，但也得慢慢开始学习调包了，这确实可以大大的提高效率。明天就要学习LoRA了，希望明天也能有所收获。期末考试也快来了，总之，加油吧！ 💪

## 📆 2025-12-26 Fri. | Day 9: 完成自己的第一个小项目

### 📝 核心进度

- 继续学习了temperature, top_k等概念
- 完成了mini-GPT小项目的搭建，编写了dataset.py, train.py, inference.py等核心文件
- 初步学习了使用tmux来进行长时间的训练, 并尝试处理了训练日志并通过复习Regex来捕获数据，画loss curve
- 同时也学习了怎样更好地展示自己的作品：plot_loss.py可视化loss curve, interactive.py, 和app.py图形页面UI
- 理解整理了.gitignore, 更深刻的理解了各个文件的作用
- 更新整理了Github的README,requirements.txt等

### 🐛 Bug Logs

- missing key(s) in state_dict -> 字典里并不存在所提的键，可能被封装进字典中的字典里 -> 进行条件判断
- too many indices for tensor of dimension 1 -> 维度对不上 -> unsuqeeze()升维/suqeeze()降维

### ☁️ 碎碎念

比较重要的一个点是现在才意识到以后README主要是用英文因为写作的，以后还是要自己多练习写这些东西，地道的英文写作，更多形式的可视化Markdown呈现, Latex写作，这些也是之后值得学习的  
确实这几天学习的主要还是如何整理一个干净的仓库，实际上感觉并不比写代码，理解代码简单，看来未来还是任重而道远啊！ 💪

## 📆 2025-12-22 Mon. | Day 8: BPE调包与工程化搭建初体验

### 📝 核心进度

- 初步跟着gemini体验了调包GPT2
- 在读取了GPT2的几个模型参数之后，gemini提议新建一个工程化文件夹，来搭建将要使用GPT2预训练参数是模型
- 目前已经重构完config.py, model.py, 期间也学到了很多整理，优化代码的技巧

### ☁️ 碎碎念

之前有段时间觉得要多把时间投入在学习代码上，但现在看来，代码能力和工程能力同等重要，这也是我现在愿意停下来完整工程化从头开始搭建的原因。毕竟这些model.py, dataset.py文件结构就是之后复现论文，做项目的预演  
之前主要在写代码，完全没有体会到vim的优越性，到现在改代码改得多了，真的发现vim是神器，效率直线上升,以后也要勤加练习，提升效率。  
明天继续重构，加油吧！ 💪 这可是我的第一个完整工程化小仓库 💪

## 📆 2025-12-21 Sun. | Day 7: 又一个toy project

### 📝 核心进度

- 用新的对话框学习搭建了mini-GPT, 用莎士比亚的一段诗训练输出了一端文本
- 再次复习巩固了切片语法，以及新学习了cat(),stack()一些操作
- 学习了几个调整点:先Norm再add, 直接用nn.Embedding来写positional encoding

## ☁️ 碎碎念

实在低估了从头写代码的难度，也更深刻地体会到做好工程化管理的重要性。同时也还是有很多新东西需要学习，逻辑结构，维度变换之类的，基础语法也仍需增强。问Gemini加上自己的感悟，问题差不多问了有15条左右了，一直搞到凌晨2点，不过最后搞出来，搞完了心里还是很舒服的 😆  
明天开始就是学期最后一周了，要开始考虑复习的事了。考试安排也渐渐都出来了，并没有想象中的很集中，所以final week可能会是一边学AI,一边复习考试。总之，加油吧！ 💪

## 📆 2025-12-20 Sat. | Day 6: 变故中的思考，修整

**mood**: 😢

### ☁️ 碎碎念

今天之前一直用的Gemini对话框崩了，之前所有的对话记录他都找不到了，现在和他对话像是在和失忆的人交流一样。没办法，简单收拾一下心情之后，只好重新再开一个对话框学习，还需要把之前的信息都同步起来，正好也给自己一个休息的时间吧，三周以来，也算学得蛮多了。  
今天还突然产生了以后要不要学AI4S的想法，简单的以为那边会不会不卷一些，稳定一些😆(哈哈哈)，了解了一些之后还是打消了这个念头,确实跟着时代风口走感觉更适合我一些，继续加油吧! 💪 auto driving, embodied AI, VLA, MMLM都可能是以后的方向，i先继续加油学着吧！

## 📆 2025-12-19 Fri. | Day 5: 学了transformer大部分

### 📝 核心进度

- 掌握了self-attention, multi-head attention机制
- 初步学习了positional_encoding,虽然原理有点没看懂，哈哈，但被直接加到token_embedding的思想震撼，更加深刻理解了映射到高维空间的思想
- 尝试搭建了transformer block和简易的transformer模型

### 💡 思考

今天更加理解了transformer的优点不仅在于通过**multi-head机制**以及变换成**矩阵运算**来充分开发适配GPU,也在于通过计算全局attention机制，来高效利用，理解全局，相较于RNN和LSTM走一步看一步有效延长文本有效长度,提升理解效果

## 📆 2025-12-17 Wed. | Day 4: 学完RNN与工程能力磨练

### 📝 核心进度

- Ubantu换了Fcitx5输入法，感觉确实比原装输入法好用一些
- VS code实现了按Esc自动切换英文，并继续精通了a, i, >C+`等无鼠标操作
- 实现了RNN,明白了文本->ID->tensor->Embedding->RNN->Linear
- 入门了传说中的Transformer,了解了self-attention的数学原理

### ☁️ 碎碎念

比起完成了RNN的学习，我觉得今天更大的收获是第一次开始更加理解与重视工程能力，更加重视输入，重视无键盘流，重视文件结构，重视代码规范。同时，这也让我意识到未来还有很长的道路需要走，不管是基本的算法，工程能力，代码能力，对领域的理解和洞察力，这些都是我未来许许多多需要去学习的地方。明天开始，就要去学习神作Transformer了，加油啊！ 💪

## 📅 2025-12-15 Mon. | Day 3: 跑通CNN与ResNet

### 📝 核心进度

- 理解了卷积层(提取特征)和全连接层(找全局关系)之间的区别和联系
- 学习了Residual Block的代码和基本思想
- 领会了厚度，特征，channels，卷积核之间的联系

💡 思考

- 有多少个out_channels就会有多少个卷积核。每个卷积核对应一个out_channel，一个特征，一种与其他卷积核不同的加权算法。
- 而卷积核用来加权的原材料就是每一个in_channels，并根据kernel_size和padding来看是否进行浓缩和尺寸变化。并且kernel_size和padding影响的只是单个channel，单个特征的精细程度。
- 对于第一层卷积层in_channel=1或3的理解，用这个厚度来理解也很好，因为才是第一层，所以也谈不上什么特征，或者说特征就是图片一个本身，而彩色图片也就算是3个图片自己的叠加而已

☁️ 碎碎念

今天终于算是学完了Residual Block，CV基础的学习也告一段落了，接下来就要进入NLP了。离学期结束还有两个星期，加油啊！💪  
今天在学习过程中还遇到了两个之前没有体会过的体验，一个就是第一次觉得模型训练的时长太久了，导致无法很好的掌控这段时间了，以后也要多家思考如何利用，可以在终端里训练，然后继续做事：另一个是意识到自己的Thinkbook 16+ 还是以CPU见长，GPU性能并不出众，以后可能要考虑租卡之类的了，现在先凑合着用吧。

## 📅 2025-12-14 Sun. | Day 2: 初识 CNN 与工程化整理

**Mood**: 😤 -> 😌 (折腾完舒服了)  

### 📝 核心进度

- 明白了全连接层的局限性（破坏空间结构）。
- 了解了卷积核（Kernel）就像手电筒一样扫描图片。
- 把 GitHub 仓库整理得像个样板房了。

### 💡 思考

原来 CNN 的核心就是“不拍扁”，保持图片的二维形状去提取特征。感觉这更符合人类的视觉逻辑。

### ☁️ 碎碎念

今天同样也是一个milestone。我创建了我的Github账号，并开始系统整理我的仓库，即使现在文件，代码量都并不多，但也已经感受到整理仓库真的十分劳神了。不过能够看着仓库变得清清爽爽，感觉做这些都是值得的。 😄  
同时我也新学习了CNN里的卷积层，池化层，初步感知了CNN的核心思想:用卷积层，池化层等工具不断提炼，优化二维图形的信息，并凭借最终高度浓缩的信息特征来推断输入图形.明天将开始写CNN在MNIST上的main.py了，一定可以完成的！ 💪

## 📅 2025-12-13 Sat. | Day 1: 搞定 MNIST 全连接网络

**Mood**: 🤩 (激动)

### 📝 核心进度

- 跑通了人生第一个神经网络(MLP)，准确率 95.19%！
- 明白了梯度清零，前向传播，计算损失，后向传播，更新参数这一套完整模型训练流程
- 彻底搞懂了 `view(-1, 28*28)` 的物理含义。

### 🐛 踩坑实录 (Bug Log)

- **问题**：`ImportError: cannot import name 'imshow'`
- **原因**：笑死，写完代码没保存 (`Ctrl+S`) 就运行了，Python 读的是空文件。
- **教训**：以后运行前先看一眼标签页有没有圆点！一定要养成肌肉记忆。

### 💭 碎碎念 (Dev Diary)

这是我选择开始Learning Log的的第一天，在这周的前一周，我入门了Ubantu，bash,vim,git等工具类，,且在本周的前段时间学习了numpy基础语法并正式入门DL，从加载数据开始学起。  
而今天这一天确实值得作为我的一个milestone了。我融汇前面所学，通过两层全连接层，加一层激活函数的极简模型，以最终95.19%的准确率完成了MINST数字识别。从此也算是可以说出"Hello DL!"了.第二天就要开始继续更深层次的学习了，继续学习CNN，也是个重重点，加油啊！💪
