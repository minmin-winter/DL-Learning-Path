
# 📔 Minmin's Deep Learning Journey

> 记录我从零开始死磕深度学习的每一天.  
> "Slow is smooth, smooth is fast."

## 📅 2025-12-15 | Day 3: 跑通CNN与ResNet

### 📝 核心进度

- 理解了卷积层(提取特征)和全连接层(找全局关系)之间的区别和联系
- 学习了Residual Block的代码和基本思想
- 领会了厚度，特征，channels，卷积核之间的联系

💡 思考

- 有多少个out_channels就会有多少个卷积核。每个卷积核对应一个out_channel，一个特征，一种与其他卷积核不同的加权算法。
- 而卷积核用来加权的原材料就是每一个in_channels，并根据kernel_size和padding来看是否进行浓缩和尺寸变化。并且kernel_size和padding影响的只是单个channel，单个特征的精细程度。
- 对于第一层卷积层in_channel=1或3的理解，用这个厚度来理解也很好，因为才是第一层，所以也谈不上什么特征，或者说特征就是图片一个本身，而彩色图片也就算是3个图片自己的叠加而已

☁️ 碎碎念

今天终于算是学完了Residual Block，CV基础的学习也告一段落了，接下来就要进入NLP了。离学期结束还有两个星期，加油啊！💪

今天在学习过程中还遇到了两个之前没有体会过的体验，一个就是第一次觉得模型训练的时长太久了，导致无法很好的掌控这段时间了，以后也要多家思考如何利用，可以在终端里训练，然后继续做事：另一个是意识到自己的Thinkbook 16+ 还是以CPU见长，GPU性能并不出众，以后可能要考虑租卡之类的了，现在先凑合着用吧。

## 📅 2025-12-14 | Day 2: 初识 CNN 与工程化整理

**Mood**: 😤 -> 😌 (折腾完舒服了)  

### 📝 核心进度

- 明白了全连接层的局限性（破坏空间结构）。
- 了解了卷积核（Kernel）就像手电筒一样扫描图片。
- 把 GitHub 仓库整理得像个样板房了。

### 💡 思考

原来 CNN 的核心就是“不拍扁”，保持图片的二维形状去提取特征。感觉这更符合人类的视觉逻辑。

### ☁️ 碎碎念

今天同样也是一个milestone。我创建了我的Github账号，并开始系统整理我的仓库，即使现在文件，代码量都并不多，但也已经感受到整理仓库真的十分劳神了。不过能够看着仓库变得清清爽爽，感觉做这些都是值得的。 😄

同时我也新新学习了CNN里的卷积层，池化层，初步感知了CNN的核心思想:用卷积层，池化层等工具不断提炼，优化二维图形的信息，并凭借最终高度浓缩的信息特征来推断输入图形.明天将开始写CNN在MNIST上的main.py了，一定可以完成的！ 💪

## 📅 2025-12-13 | Day 1: 搞定 MNIST 全连接网络

**Mood**: 🤩 (激动)

### 📝 核心进度

- 跑通了人生第一个神经网络(MLP)，准确率 95.19%！
- 明白了梯度清零，前向传播，计算损失，后向传播，更新参数这一套完整模型训练流程
- 彻底搞懂了 `view(-1, 28*28)` 的物理含义。

### 🐛 踩坑实录 (Bug Log)

- **问题**：`ImportError: cannot import name 'imshow'`
- **原因**：笑死，写完代码没保存 (`Ctrl+S`) 就运行了，Python 读的是空文件。
- **教训**：以后运行前先看一眼标签页有没有圆点！一定要养成肌肉记忆。

### 💭 碎碎念 (Dev Diary)

这是我选择开始Learning Log的的第一天，在这周的前一周，我入门了Ubantu，bash,vim,git等工具类，,且在本周的前段时间学习了numpy基础语法并正式入门DL，从加载数据开始学起。

而今天这一天确实值得作为我的一个milestone了。我融汇前面所学，通过两层全连接层，加一层激活函数的极简模型，以最终95.19%的准确率完成了MINST数字识别。从此也算是可以说出"Hello DL!"了.第二天就要开始继续更深层次的学习了，继续学习CNN，也是个重重点，加油啊！💪
