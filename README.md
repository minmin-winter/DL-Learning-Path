# ğŸš€ Deep Learning Learning Path

> My journey from Math to AI: Implementing Deep Learning models from scratch.

[![Profile](https://img.shields.io/badge/GitHub-Minmin_Winter-black?logo=github)](https://github.com/minmin-winter)

Welcome! This repository documents my code and learning notes as I dive into Deep Learning. My goal is to understand the "magic" behind modern AI by building everything from the ground up.

## ğŸŒŸ Highlight Project: Mini-GPT

I recently built a Transformer-based Language Model from scratch (no HuggingFace for the model architecture!).
ğŸ‘‰ **[Check out the Mini-GPT Project](./05_mini_gpt_project)**

* **Architecture**: Decoder-only Transformer with Self-Attention.
* **Tokenizer**: BPE (Byte Pair Encoding).
* **Performance**: Loss converged to 2.78 on Shakespeare dataset.
* **Demo**: Interactive Web UI available.

## ğŸ—ºï¸ Project Roadmap

### Phase 1: The Foundation âœ…

- [x] **00_numpy_warmup**: Understanding Backpropagation with raw NumPy.
- [x] **01_mnist_demo**: MLP for handwritten digits recognition (95%+ accuracy).

### Phase 2: Computer Vision Basics ğŸš§

- [x] **02_cnn_basics**: Implemented Convolution & Pooling layers.
- [x] Modern CNNs (ResNet implementation).

### Phase 3: Natural Language Processing Basics ğŸ”¥

- [x] **03_rnn_basics**: Simple RNN & LSTM for sequence data.
- [x] **04_transformer_basics**: Self-Attention & Multi-Head Attention & Masked Attention math, Positional Encoding, Transfomer Block.
- [x] **05_mini_gpt_project**: **(Current Best)** A complete GPT implementation.

## ğŸ› ï¸ Tech Stack

* **Language**: Python 3.9
* **Framework**: PyTorch
* **Tools**: Gradio, Matplotlib, NumPy
