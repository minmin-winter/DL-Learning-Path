import os
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# 1. è‡ªåŠ¨æ‰¾åˆ°æœ€æ–°çš„æ¨¡å‹å­˜æ¡£ (Checkpoint)
output_dir = "./models"

# æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
if not os.path.exists(output_dir):
    raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç›®å½•: {output_dir}ï¼Œè¯·ç¡®è®¤ä½ å·²ç»è¿è¡Œè¿‡è®­ç»ƒè„šæœ¬ï¼")

# æ‰¾åˆ°é‡Œé¢å« 'checkpoint-xxx' çš„æ–‡ä»¶å¤¹
checkpoints = [d for d in os.listdir(output_dir) if d.startswith("checkpoint")]
if not checkpoints:
    raise FileNotFoundError("åœ¨ waimai_model é‡Œæ²¡æ‰¾åˆ° checkpoint æ–‡ä»¶å¤¹ï¼")

# æ’åºï¼Œå–æ•°å­—æœ€å¤§çš„é‚£ä¸ªï¼ˆä¹Ÿå°±æ˜¯è®­ç»ƒæœ€ä¹…çš„ï¼‰
latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))[-1]
model_path = os.path.join(output_dir, latest_checkpoint)

print(f"ğŸŒŸ æ­£åœ¨åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹: {model_path} ...")

# 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
# æ³¨æ„ï¼šæˆ‘ä»¬åŠ è½½çš„æ˜¯â€œå¾®è°ƒåâ€çš„æƒé‡
model = GPT2LMHeadModel.from_pretrained(model_path)
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token # åˆ«å¿˜äº†è¿™ä¸€æ­¥ï¼Œä¿æŒä¸€è‡´

# 3. å‡†å¤‡æµ‹è¯•è¾“å…¥
# æˆ‘ä»¬ç”¨ä¸­æ–‡å¼€å¤´ï¼Œçœ‹çœ‹å®ƒèƒ½ä¸èƒ½æ¥ä¸­æ–‡
prompt = "å‘³é“" 
print(f"\n[ç”¨æˆ·è¾“å…¥]: {prompt}")

inputs = tokenizer(prompt, return_tensors="pt")

# 4. ç”Ÿæˆ (Inference)
model.eval()
print("æ­£åœ¨ç”Ÿæˆä¸­...")

with torch.no_grad():
    outputs = model.generate(
        inputs["input_ids"], 
        max_length=50, 
        do_sample=True, 
        temperature=0.7, # æ¸©åº¦ä½ä¸€ç‚¹ï¼Œè®©å®ƒèƒŒè¯µå¾—å‡†ä¸€ç‚¹
        top_k=50,
        pad_token_id=tokenizer.eos_token_id
    )

# 5. è§£ç 
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("-" * 30)
print("[å¤–å– GPT ç”Ÿæˆç»“æœ]:")
print(generated_text)
print("-" * 30)